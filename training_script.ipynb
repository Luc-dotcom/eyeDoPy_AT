{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Train your own object detector with Faster-RCNN & PyTorch: Heads detector"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename downloaded files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from utils import get_filenames_of_path\n",
    "\n",
    "root = pathlib.Path('heads')\n",
    "\n",
    "inputs = get_filenames_of_path(root / 'input')\n",
    "inputs.sort()\n",
    "\n",
    "\n",
    "for idx, path in enumerate(inputs):\n",
    "    old_name = path.stem\n",
    "    old_extension = path.suffix\n",
    "    dir = path.parent\n",
    "    new_name = str(idx).zfill(3) + old_extension\n",
    "    path.rename(pathlib.Path(dir, new_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start annotating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from visual import Annotator\n",
    "\n",
    "from utils import get_filenames_of_path\n",
    "\n",
    "dir = pathlib.Path('heads')\n",
    "image_files = get_filenames_of_path(dir / 'input')\n",
    "\n",
    "annotator = Annotator(image_ids=image_files)\n",
    "annotator.napari()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator.add_class(label='head', color='red') # head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator.add_class(label='eye', color='blue') # eye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the annotations of the current image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator.export(pathlib.Path('.../some_directory'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save all available annotations in one go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator.export_all(pathlib.Path('heads/target'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import torch\n",
    "from utils import get_filenames_of_path\n",
    "\n",
    "root = pathlib.Path('heads')\n",
    "\n",
    "targets = get_filenames_of_path(root / 'target')\n",
    "targets.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation = torch.load(targets[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation['boxes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "\n",
    "from datasets import ObjectDetectionDataSet\n",
    "from transformations import ComposeDouble, Clip, AlbumentationWrapper, FunctionWrapperDouble\n",
    "from transformations import normalize_01\n",
    "from utils import get_filenames_of_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = pathlib.Path('heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = get_filenames_of_path(root / 'input')\n",
    "targets = get_filenames_of_path(root / 'target')\n",
    "\n",
    "inputs.sort()\n",
    "targets.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    'head': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = ComposeDouble([\n",
    "    Clip(),\n",
    "    # AlbumentationWrapper(albumentation=A.HorizontalFlip(p=0.5)),\n",
    "    # AlbumentationWrapper(albumentation=A.RandomScale(p=0.5, scale_limit=0.5)),\n",
    "    # AlbuWrapper(albu=A.VerticalFlip(p=0.5)),\n",
    "    FunctionWrapperDouble(np.moveaxis, source=-1, destination=0),\n",
    "    FunctionWrapperDouble(normalize_01)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ObjectDetectionDataSet(inputs=inputs,\n",
    "                                 targets=targets,\n",
    "                                 transform=transforms,\n",
    "                                 use_cache=False,\n",
    "                                 convert_to_format=None,\n",
    "                                 mapping=mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_mapping = {\n",
    "    1: 'red',\n",
    "}\n",
    "\n",
    "from visual import DatasetViewer\n",
    "\n",
    "datasetviewer = DatasetViewer(dataset, color_mapping)\n",
    "datasetviewer.napari()\n",
    "datasetviewer.gui_text_properties(datasetviewer.shape_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize dataset with Faster-RCNN transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_mapping = {\n",
    "    1: 'red',\n",
    "}\n",
    "\n",
    "from visual import DatasetViewer\n",
    "\n",
    "from torchvision.models.detection.transform import GeneralizedRCNNTransform\n",
    "\n",
    "transform = GeneralizedRCNNTransform(min_size=1024,\n",
    "                                     max_size=1024,\n",
    "                                     image_mean=[0.485, 0.456, 0.406],\n",
    "                                     image_std=[0.229, 0.224, 0.225])\n",
    "\n",
    "datasetviewer = DatasetViewer(dataset, color_mapping, rccn_transform=transform)\n",
    "datasetviewer.napari()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetviewer.image_layer.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import stats_dataset\n",
    "\n",
    "stats = stats_dataset(dataset)\n",
    "\n",
    "from torchvision.models.detection.transform import GeneralizedRCNNTransform\n",
    "\n",
    "transform = GeneralizedRCNNTransform(min_size=1024,\n",
    "                                     max_size=1024,\n",
    "                                     image_mean=[0.485, 0.456, 0.406],\n",
    "                                     image_std=[0.229, 0.224, 0.225])\n",
    "\n",
    "stats_transform = stats_dataset(dataset, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats['image_height'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_transform['image_height'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats['image_height'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_transform['image_height'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AnchorViewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.transform import GeneralizedRCNNTransform\n",
    "from visual import AnchorViewer\n",
    "\n",
    "transform = GeneralizedRCNNTransform(min_size=1024,\n",
    "                                     max_size=1024,\n",
    "                                     image_mean=[0.485, 0.456, 0.406],\n",
    "                                     image_std=[0.229, 0.224, 0.225])\n",
    "\n",
    "image = dataset[0]['x']  # ObjectDetectionDataSet\n",
    "feature_map_size = (512, 32, 32)\n",
    "anchorviewer = AnchorViewer(image=image,\n",
    "                 rcnn_transform=transform,\n",
    "                 feature_map_size=feature_map_size,\n",
    "                 anchor_size=((128, 256, 512),),\n",
    "                 aspect_ratios=((0.5, 1.0, 2.0),)\n",
    "                 )\n",
    "anchorviewer.napari()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pathlib\n",
    "\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import ObjectDetectionDataSet\n",
    "from transformations import ComposeDouble, Clip, AlbumentationWrapper, FunctionWrapperDouble\n",
    "from transformations import normalize_01\n",
    "from utils import get_filenames_of_path, collate_double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "params = {'BATCH_SIZE': 2,\n",
    "          'LR': 0.001,\n",
    "          'PRECISION': 32,\n",
    "          'CLASSES': 2,\n",
    "          'SEED': 42,\n",
    "          'PROJECT': 'Heads',\n",
    "          'EXPERIMENT': 'heads',\n",
    "          'MAXEPOCHS': 500,\n",
    "          'BACKBONE': 'resnet34',\n",
    "          'FPN': False,\n",
    "          'ANCHOR_SIZE': ((32, 64, 128, 256, 512),),\n",
    "          'ASPECT_RATIOS': ((0.5, 1.0, 2.0),),\n",
    "          'MIN_SIZE': 1024,\n",
    "          'MAX_SIZE': 1024,\n",
    "          'IMG_MEAN': [0.485, 0.456, 0.406],\n",
    "          'IMG_STD': [0.229, 0.224, 0.225],\n",
    "          'IOU_THRESHOLD': 0.5\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root directory\n",
    "root = pathlib.Path('heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input and target files\n",
    "inputs = get_filenames_of_path(root / 'input')\n",
    "targets = get_filenames_of_path(root / 'target')\n",
    "\n",
    "inputs.sort()\n",
    "targets.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping\n",
    "mapping = {\n",
    "    'head': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training transformations and augmentations\n",
    "transforms_training = ComposeDouble([\n",
    "    Clip(),\n",
    "    AlbumentationWrapper(albumentation=A.HorizontalFlip(p=0.5)),\n",
    "    AlbumentationWrapper(albumentation=A.RandomScale(p=0.5, scale_limit=0.5)),\n",
    "    # AlbuWrapper(albu=A.VerticalFlip(p=0.5)),\n",
    "    FunctionWrapperDouble(np.moveaxis, source=-1, destination=0),\n",
    "    FunctionWrapperDouble(normalize_01)\n",
    "])\n",
    "\n",
    "# validation transformations\n",
    "transforms_validation = ComposeDouble([\n",
    "    Clip(),\n",
    "    FunctionWrapperDouble(np.moveaxis, source=-1, destination=0),\n",
    "    FunctionWrapperDouble(normalize_01)\n",
    "])\n",
    "\n",
    "# test transformations\n",
    "transforms_test = ComposeDouble([\n",
    "    Clip(),\n",
    "    FunctionWrapperDouble(np.moveaxis, source=-1, destination=0),\n",
    "    FunctionWrapperDouble(normalize_01)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "seed_everything(params['SEED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training validation test split\n",
    "inputs_train, inputs_valid, inputs_test = inputs[:12], inputs[12:16], inputs[16:]\n",
    "targets_train, targets_valid, targets_test = targets[:12], targets[12:16], targets[16:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset training\n",
    "dataset_train = ObjectDetectionDataSet(inputs=inputs_train,\n",
    "                                       targets=targets_train,\n",
    "                                       transform=transforms_training,\n",
    "                                       use_cache=True,\n",
    "                                       convert_to_format=None,\n",
    "                                       mapping=mapping)\n",
    "\n",
    "# dataset validation\n",
    "dataset_valid = ObjectDetectionDataSet(inputs=inputs_valid,\n",
    "                                       targets=targets_valid,\n",
    "                                       transform=transforms_validation,\n",
    "                                       use_cache=True,\n",
    "                                       convert_to_format=None,\n",
    "                                       mapping=mapping)\n",
    "\n",
    "# dataset test\n",
    "dataset_test = ObjectDetectionDataSet(inputs=inputs_test,\n",
    "                                      targets=targets_test,\n",
    "                                      transform=transforms_test,\n",
    "                                      use_cache=True,\n",
    "                                      convert_to_format=None,\n",
    "                                      mapping=mapping)\n",
    "\n",
    "# dataloader training\n",
    "dataloader_train = DataLoader(dataset=dataset_train,\n",
    "                              batch_size=params['BATCH_SIZE'],\n",
    "                              shuffle=True,\n",
    "                              num_workers=0,\n",
    "                              collate_fn=collate_double)\n",
    "\n",
    "# dataloader validation\n",
    "dataloader_valid = DataLoader(dataset=dataset_valid,\n",
    "                              batch_size=1,\n",
    "                              shuffle=False,\n",
    "                              num_workers=0,\n",
    "                              collate_fn=collate_double)\n",
    "\n",
    "# dataloader test\n",
    "dataloader_test = DataLoader(dataset=dataset_test,\n",
    "                             batch_size=1,\n",
    "                             shuffle=False,\n",
    "                             num_workers=0,\n",
    "                             collate_fn=collate_double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neptune logger\n",
    "from pytorch_lightning.loggers.neptune import NeptuneLogger\n",
    "from api_key_neptune import get_api_key\n",
    "\n",
    "# api_key_neptune.py\n",
    "#\n",
    "# def get_api_key():\n",
    "#     return 'your_super_long_API_token'\n",
    "\n",
    "\n",
    "api_key = get_api_key()\n",
    "\n",
    "neptune_logger = NeptuneLogger(\n",
    "    api_key=api_key,\n",
    "    project_name=f'your_neptune_name/{params[\"PROJECT\"]}',\n",
    "    experiment_name=params['EXPERIMENT'],\n",
    "    params=params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model init\n",
    "from faster_RCNN import get_fasterRCNN_resnet\n",
    "\n",
    "model = get_fasterRCNN_resnet(num_classes=params['CLASSES'],\n",
    "                              backbone_name=params['BACKBONE'],\n",
    "                              anchor_size=params['ANCHOR_SIZE'],\n",
    "                              aspect_ratios=params['ASPECT_RATIOS'],\n",
    "                              fpn=params['FPN'],\n",
    "                              min_size=params['MIN_SIZE'],\n",
    "                              max_size=params['MAX_SIZE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightning init\n",
    "from faster_RCNN import FasterRCNN_lightning\n",
    "\n",
    "task = FasterRCNN_lightning(model=model, lr=params['LR'], iou_threshold=params['IOU_THRESHOLD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor, EarlyStopping\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(monitor='Validation_mAP', mode='max')\n",
    "learningrate_callback = LearningRateMonitor(logging_interval='step', log_momentum=False)\n",
    "early_stopping_callback = EarlyStopping(monitor='Validation_mAP', patience=50, mode='max')\n",
    "\n",
    "# trainer init\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "trainer = Trainer(gpus=1,\n",
    "                  precision=params['PRECISION'],  # try 16 with enable_pl_optimizer=False\n",
    "                  callbacks=[checkpoint_callback, learningrate_callback, early_stopping_callback],\n",
    "                  default_root_dir='heads',  # where checkpoints are saved to\n",
    "                  logger=neptune_logger,\n",
    "                  log_every_n_steps=1,\n",
    "                  num_sanity_val_steps=0,\n",
    "                  enable_pl_optimizer=False,  # False seems to be necessary for half precision\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n",
    "trainer.max_epochs = params['MAXEPOCHS']\n",
    "trainer.fit(task,\n",
    "            train_dataloader=dataloader_train,\n",
    "            val_dataloaders=dataloader_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start testing\n",
    "trainer.test(ckpt_path='best', test_dataloaders=dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log packages\n",
    "from utils import log_packages_neptune\n",
    "\n",
    "log_packages_neptune(neptune_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log mapping as table\n",
    "from utils import log_mapping_neptune\n",
    "\n",
    "log_mapping_neptune(mapping, neptune_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log model\n",
    "from utils import log_model_neptune\n",
    "\n",
    "checkpoint_path = pathlib.Path(checkpoint_callback.best_model_path)\n",
    "log_model_neptune(checkpoint_path=checkpoint_path,\n",
    "                  save_directory=pathlib.Path.home(),\n",
    "                  name='best_model.pt',\n",
    "                  neptune_logger=neptune_logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import ast\n",
    "import pathlib\n",
    "\n",
    "import neptune\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from api_key_neptune import get_api_key\n",
    "from datasets import ObjectDetectionDatasetSingle, ObjectDetectionDataSet\n",
    "from transformations import ComposeSingle, FunctionWrapperSingle, normalize_01, ComposeDouble, FunctionWrapperDouble\n",
    "from utils import get_filenames_of_path, collate_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "params = {'EXPERIMENT': 'experiment_name',\n",
    "          'INPUT_DIR': 'heads/test', # files to predict\n",
    "          'PREDICTIONS_PATH': 'heads', # where to save the predictions\n",
    "          'MODEL_DIR': 'heads', # load model from checkpoint\n",
    "          'DOWNLOAD': False, # whether to download from neptune\n",
    "          'DOWNLOAD_PATH': 'heads/', # where to save the model\n",
    "          'OWNER': 'your_neptune_name',\n",
    "          'PROJECT': 'Heads',\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input files\n",
    "inputs = get_filenames_of_path(pathlib.Path(params['INPUT_DIR']))\n",
    "inputs.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformations\n",
    "transforms = ComposeSingle([\n",
    "    FunctionWrapperSingle(np.moveaxis, source=-1, destination=0),\n",
    "    FunctionWrapperSingle(normalize_01)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset and dataloader\n",
    "dataset = ObjectDetectionDatasetSingle(inputs=inputs,\n",
    "                                       transform=transforms,\n",
    "                                       use_cache=False,\n",
    "                                       )\n",
    "\n",
    "dataloader_prediction = DataLoader(dataset=dataset,\n",
    "                                   batch_size=1,\n",
    "                                   shuffle=False,\n",
    "                                   num_workers=0,\n",
    "                                   collate_fn=collate_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import experiment from neptune\n",
    "api_key = get_api_key()  # get the personal api key\n",
    "project_name = f'{params[\"OWNER\"]}/{params[\"PROJECT\"]}'\n",
    "project = neptune.init(project_qualified_name=project_name, api_token=api_key)  # get project\n",
    "experiment_id = params['EXPERIMENT']  # experiment id\n",
    "experiment = project.get_experiments(id=experiment_id)[0]\n",
    "parameters = experiment.get_parameters()\n",
    "properties = experiment.get_properties()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view dataset\n",
    "from visual import DatasetViewerSingle\n",
    "from torchvision.models.detection.transform import GeneralizedRCNNTransform\n",
    "\n",
    "transform = GeneralizedRCNNTransform(min_size=int(parameters['MIN_SIZE']),\n",
    "                                     max_size=int(parameters['MAX_SIZE']),\n",
    "                                     image_mean=ast.literal_eval(parameters['IMG_MEAN']),\n",
    "                                     image_std=ast.literal_eval(parameters['IMG_STD']))\n",
    "\n",
    "\n",
    "datasetviewer = DatasetViewerSingle(dataset, rccn_transform=None)\n",
    "datasetviewer.napari()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download model from neptune or load from checkpoint\n",
    "if params['DOWNLOAD']:\n",
    "    download_path = pathlib.Path(params['DOWNLOAD_PATH'])\n",
    "    model_name = properties['checkpoint_name'] # logged when called log_model_neptune()\n",
    "    if not (download_path / model_name).is_file():\n",
    "        experiment.download_artifact(path=model_name, destination_dir=download_path)  # download model\n",
    "\n",
    "    model_state_dict = torch.load(download_path / model_name)\n",
    "else:\n",
    "    checkpoint = torch.load(params['MODEL_DIR'])\n",
    "    model_state_dict = checkpoint['hyper_parameters']['model'].state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model init\n",
    "from faster_RCNN import get_fasterRCNN_resnet\n",
    "model = get_fasterRCNN_resnet(num_classes=int(parameters['CLASSES']),\n",
    "                              backbone_name=parameters['BACKBONE'],\n",
    "                              anchor_size=ast.literal_eval(parameters['ANCHOR_SIZE']),\n",
    "                              aspect_ratios=ast.literal_eval(parameters['ASPECT_RATIOS']),\n",
    "                              fpn=ast.literal_eval(parameters['FPN']),\n",
    "                              min_size=int(parameters['MIN_SIZE']),\n",
    "                              max_size=int(parameters['MAX_SIZE'])\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weights\n",
    "model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "model.eval()\n",
    "for sample in dataloader_prediction:\n",
    "    x, x_name = sample\n",
    "    with torch.no_grad():\n",
    "        pred = model(x)\n",
    "        pred = {key: value.numpy() for key, value in pred[0].items()}\n",
    "        name = pathlib.Path(x_name[0])\n",
    "        torch.save(pred, pathlib.Path(params['PREDICTIONS_PATH']) / name.with_suffix('.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create prediction dataset\n",
    "predictions = get_filenames_of_path(pathlib.Path(params['PREDICTIONS_PATH']))\n",
    "predictions.sort()\n",
    "\n",
    "transforms_prediction = ComposeDouble([\n",
    "    FunctionWrapperDouble(np.moveaxis, source=-1, destination=0),\n",
    "    FunctionWrapperDouble(normalize_01)\n",
    "])\n",
    "\n",
    "dataset_prediction = ObjectDetectionDataSet(inputs=inputs,\n",
    "                                            targets=predictions,\n",
    "                                            transform=transforms_prediction,\n",
    "                                            use_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize predictions\n",
    "from visual import DatasetViewer\n",
    "\n",
    "color_mapping = {\n",
    "    1: 'red',\n",
    "}\n",
    "\n",
    "datasetviewer_prediction = DatasetViewer(dataset_prediction, color_mapping)\n",
    "datasetviewer_prediction.napari()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add text properties gui\n",
    "datasetviewer_prediction.gui_text_properties(datasetviewer_prediction.shape_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add nms slider\n",
    "datasetviewer_prediction.gui_nms_slider(datasetviewer_prediction.shape_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add score slider\n",
    "datasetviewer_prediction.gui_score_slider(datasetviewer_prediction.shape_layer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "test_env",
   "language": "python",
   "display_name": "Python (test_env)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}