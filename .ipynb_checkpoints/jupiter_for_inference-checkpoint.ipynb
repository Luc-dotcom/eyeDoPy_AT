{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    " \n",
    "# imports\n",
    "import ast\n",
    "import pathlib\n",
    " \n",
    "import neptune\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    " \n",
    "from api_key_neptune import get_api_key\n",
    "from datasets import ObjectDetectionDatasetSingle, ObjectDetectionDataSet\n",
    "from transformations import ComposeSingle, FunctionWrapperSingle, normalize_01, ComposeDouble, FunctionWrapperDouble\n",
    "from utils import get_filenames_of_path, collate_single\n",
    "from IPython import get_ipython\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import get_ipython\n",
    "\n",
    "get_ipython().magic(u'matplotlib inline')\n",
    " \n",
    "# parameters\n",
    "params = {'EXPERIMENT': 'heads',\n",
    "          'INPUT_DIR': './heads/test', # files to predict\n",
    "          'PREDICTIONS_PATH': './predictions', # where to save the predictions\n",
    "          'MODEL_DIR': './epoch=150-step=28387.ckpt', # load model from checkpoint\n",
    "          'DOWNLOAD': False, # wether to download from neptune\n",
    "          'DOWNLOAD_PATH': './savedModel', # where to save the model\n",
    "          'OWNER': 'luca76485743',\n",
    "          'PROJECT': 'Heads',\n",
    "          'MIN_SIZE': 657,\n",
    "          'FPN': 'False',\n",
    "          'MAX_SIZE': 876,\n",
    "          'IMG_MEAN': '[0.485, 0.456, 0.406]',\n",
    "          'IMG_STD': '[0.229, 0.224, 0.225]',\n",
    "          'CLASSES': 2, # 5 for all classes\n",
    "          'BACKBONE': 'resnet34',\n",
    "          'ANCHOR_SIZE': '((32, 64, 128, 256, 512),)',\n",
    "          'ASPECT_RATIOS': '((0.5, 1.0, 2.0),)'\n",
    "          }\n",
    " \n",
    "# input files\n",
    "inputs = get_filenames_of_path(pathlib.Path(params['INPUT_DIR']))\n",
    "inputs.sort()\n",
    " \n",
    "# transformations\n",
    "transforms = ComposeSingle([\n",
    "    FunctionWrapperSingle(np.moveaxis, source=-1, destination=0),\n",
    "    FunctionWrapperSingle(normalize_01)\n",
    "])\n",
    " \n",
    "# create dataset and dataloader\n",
    "dataset = ObjectDetectionDatasetSingle(inputs=inputs,\n",
    "                                       transform=transforms,\n",
    "                                       use_cache=False,\n",
    "                                       )\n",
    " \n",
    "dataloader_prediction = DataLoader(dataset=dataset,\n",
    "                                   batch_size=1,\n",
    "                                   shuffle=False,\n",
    "                                   num_workers=0,\n",
    "                                   collate_fn=collate_single)\n",
    " \n",
    " \n",
    "# view dataset\n",
    "from visual import DatasetViewerSingle\n",
    "from torchvision.models.detection.transform import GeneralizedRCNNTransform\n",
    " \n",
    "transform = GeneralizedRCNNTransform(min_size=int(params['MIN_SIZE']),\n",
    "                                     max_size=int(params['MAX_SIZE']),\n",
    "                                     image_mean=ast.literal_eval(params['IMG_MEAN']),\n",
    "                                     image_std=ast.literal_eval(params['IMG_STD']))\n",
    " \n",
    " \n",
    "datasetviewer = DatasetViewerSingle(dataset, rccn_transform=None)\n",
    "#datasetviewer.napari()\n",
    " \n",
    "# download model from neptune or load from checkpoint\n",
    "if params['DOWNLOAD']:\n",
    "    download_path = pathlib.Path(params['DOWNLOAD_PATH'])\n",
    "    model_name = properties['checkpoint_name'] # logged when called log_model_neptune()\n",
    "    if not (download_path / model_name).is_file():\n",
    "        experiment.download_artifact(path=model_name, destination_dir=download_path)  # download model\n",
    " \n",
    "    model_state_dict = torch.load(download_path / model_name)\n",
    "else:\n",
    "    checkpoint = torch.load(params['MODEL_DIR'])\n",
    "    model_state_dict = checkpoint['hyper_parameters']['model'].state_dict()\n",
    " \n",
    "# model init\n",
    "from faster_RCNN import get_fasterRCNN_resnet\n",
    "model = get_fasterRCNN_resnet(num_classes=int(params['CLASSES']),\n",
    "                              backbone_name=params['BACKBONE'],\n",
    "                              anchor_size=ast.literal_eval(params['ANCHOR_SIZE']),\n",
    "                              aspect_ratios=ast.literal_eval(params['ASPECT_RATIOS']),\n",
    "                              fpn=ast.literal_eval(params['FPN']),\n",
    "                              min_size=int(params['MIN_SIZE']),\n",
    "                              max_size=int(params['MAX_SIZE'])\n",
    "                              )\n",
    " \n",
    "# load weights\n",
    "model.load_state_dict(model_state_dict)\n",
    " \n",
    "# inference\n",
    "model.eval()\n",
    "\n",
    "# delete old prediction folder\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "\n",
    "try:\n",
    "    shutil.rmtree('./predictions')\n",
    "except OSError as e:\n",
    "    print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
    "\n",
    "# create predictions folder\n",
    "\n",
    "os.mkdir(params['PREDICTIONS_PATH'])\n",
    "\n",
    "\n",
    "# generate predictions\n",
    "for sample in dataloader_prediction:\n",
    "    x, x_name = sample\n",
    "    with torch.no_grad():\n",
    "        pred = model(x)\n",
    "        pred = {key: value.numpy() for key, value in pred[0].items()}\n",
    "        name = pathlib.Path(x_name[0])\n",
    "        torch.save(pred, pathlib.Path(params['PREDICTIONS_PATH']) / name.with_suffix('.pt'))\n",
    " \n",
    "# create prediction dataset\n",
    "predictions = get_filenames_of_path(pathlib.Path(params['PREDICTIONS_PATH']))\n",
    "predictions.sort()\n",
    " \n",
    "transforms_prediction = ComposeDouble([\n",
    "    FunctionWrapperDouble(np.moveaxis, source=-1, destination=0),\n",
    "    FunctionWrapperDouble(normalize_01)\n",
    "])\n",
    " \n",
    "dataset_prediction = ObjectDetectionDataSet(inputs=inputs,\n",
    "                                            targets=predictions,\n",
    "                                            transform=transforms_prediction,\n",
    "                                            use_cache=False)\n",
    " \n",
    "# visualize predictions\n",
    "from visual import DatasetViewer\n",
    " \n",
    "color_mapping = {\n",
    "    1: 'red',\n",
    "}\n",
    " \n",
    "datasetviewer_prediction = DatasetViewer(dataset_prediction, color_mapping)\n",
    "datasetviewer_prediction.napari()\n",
    "d = datasetviewer_prediction\n",
    "d.gui_score_slider(d.shape_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
